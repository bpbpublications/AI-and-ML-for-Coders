{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnAgBSMOHnkQ"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "2NkohBWeIM3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unpack the dataset\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "werhLdc7bXFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to data\n",
        "train_path = \"aclImdb/train/\"\n",
        "test_path = \"aclImdb/test/\"\n",
        "\n",
        "# Define parameters\n",
        "max_words = 10000\n",
        "max_len = 200\n",
        "batch_size = 32\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "-CSIqF9fbf1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read data from files\n",
        "def read_data(path):\n",
        "    \"\"\"\n",
        "    Reads data from the specified path and returns lists of reviews and labels.\n",
        "\n",
        "    Args:\n",
        "        path: Path to the data directory.\n",
        "\n",
        "    Returns:\n",
        "        reviews: List of movie reviews.\n",
        "        labels: List of sentiment labels (0 for negative, 1 for positive).\n",
        "    \"\"\"\n",
        "\n",
        "    # import libraries\n",
        "    import os\n",
        "\n",
        "    # create variables for returning\n",
        "    # reviews and their labels\n",
        "    reviews = []\n",
        "    labels = []\n",
        "\n",
        "    # Reading the data\n",
        "    for sentiment in [\"pos\", \"neg\"]:\n",
        "        sentiment_path = path + sentiment + \"/\"\n",
        "        for filename in os.listdir(sentiment_path):\n",
        "            with open(os.path.join(sentiment_path, filename), \"r\", encoding=\"utf-8\") as f:\n",
        "                reviews.append(f.read())\n",
        "                labels.append(1 if sentiment == \"pos\" else 0)\n",
        "\n",
        "    # return the reviews and their labels\n",
        "    return reviews, labels\n"
      ],
      "metadata": {
        "id": "lauBYljybm3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read training and testing data\n",
        "train_reviews, train_labels = read_data(train_path)\n",
        "test_reviews, test_labels = read_data(test_path)"
      ],
      "metadata": {
        "id": "VHJlwGb_bp7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess text data\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_reviews)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_reviews)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_reviews)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_len)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_len)"
      ],
      "metadata": {
        "id": "MtVJ0RHHbuwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and validation sets\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(train_padded, train_labels, test_size=0.2)\n",
        "\n",
        "# Create the model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(max_words, 128, input_length=max_len),\n",
        "    keras.layers.LSTM(64),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "rukEkYHLb6tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train_labels = np.array(train_labels)\n",
        "val_labels = np.array(val_labels)\n",
        "history = model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(val_data, val_labels))"
      ],
      "metadata": {
        "id": "fH-0_IaTcgFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_labels = np.array(test_labels)\n",
        "test_loss, test_acc = model.evaluate(test_padded, test_labels)\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "id": "kWS5Vho9clXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot epochs vs loss\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QK2hEmqfc1yG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}